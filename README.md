# Vision transformer from scratch

This notebook is my personal implementation of the vision transformer from scratch using the research paper An Image is Worth **16x16 Words: Transformers for Image Recognition at Scale (ViT paper)** with PyTorch.

This was created using various coding practices and techniques acquired to make the code clear and eloquent.

The dataset used is a subset of the Food101 dataset with three classes Pizza, sushi and steak.

In this notebook, we can see a from-scratch implementation of a vanilla-transformer architecture and how does this work with the classification task among the above classes.

We would then see a transfer-learning based Vision Transformer model and compare the results of classification using this transfer learning model.

